<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="å°¹ç¥ºç¿”">





<title>æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æ¦‚è¿° | yqxBlog</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
            <!-- MathJaxé…ç½®ï¼Œå¯é€šè¿‡å•ç¾å…ƒç¬¦å·ä¹¦å†™è¡Œå†…å…¬å¼ç­‰ -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- ç»™MathJaxå…ƒç´ æ·»åŠ has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- é€šè¿‡è¿æ¥CDNåŠ è½½MathJaxçš„jsä»£ç  -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


        
    


<meta name="generator" content="Hexo 5.4.2">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "Â· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "Â· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">BUPTyqx&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">BUPTyqx&#39;s Blog</a><a id="mobile-toggle-theme">Â·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // ä¸º 6 æ—¶å±•å¼€æ‰€æœ‰
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // è¿™ä¸ªå€¼æ˜¯ç”± tocbot æºç é‡Œå®šä¹‰çš„ scrollSmoothDuration å¾—æ¥çš„
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">æ–‡æœ¬åˆ†ç±»ä»»åŠ¡æ¦‚è¿°</h1>
            
                <div class="post-meta">
                    
                        ğŸ¦¹â€â™€ï¸ä½œè€…: <a itemprop="author" rel="author" href="/">å°¹ç¥ºç¿”</a><br>
                    

                    
                        <span class="post-time">
                        â²ï¸æ—¶é—´: <a href="#">August 16, 2022&nbsp;&nbsp;15:02:04</a><br>
                        </span>
                    
                    
                        <span class="post-category">
                            ğŸ“’ç›®å½•:
                            
                                <a href="/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">è‡ªç„¶è¯­è¨€å¤„ç†</a><br>
                            
                        </span>
                    
                    
                    
                        <span class="post-count">
                            ğŸ“‘å­—æ•°:
                        <a href="">3,124</a><br>  
                        </span>
                    
                    
                        <span class="post-count">
                    â°é¢„è®¡é˜…è¯»æ—¶é—´:
                        <a href="">19min</a>  
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <h1 id="Text-Classification"><a href="#Text-Classification" class="headerlink" title="Text Classification"></a>Text Classification</h1><h2 id="1-æ¦‚è¿°"><a href="#1-æ¦‚è¿°" class="headerlink" title="1. æ¦‚è¿°"></a>1. æ¦‚è¿°</h2><ul>
<li>Text Classification is a technique of <strong>categorizing</strong> natural language texts into <strong>pre-defined</strong>, organized <strong>groups</strong></li>
<li>In other words - it is the activity of labeling texts with categories from a pre-defined set, based upon the content</li>
<li><p>Classic examples include classification of books in libraries or segmentation of articles in news,by looking at the content.</p>
</li>
<li><p>Text classification is a sub-field of <strong>text analytics</strong>, which uses <strong>machine learning</strong> to extract meaning from text documents</p>
</li>
<li>Text classification technique has been used successfully for:<ul>
<li>Sentiment analysis</li>
<li>Topic detection</li>
<li>Language detection</li>
<li>Fraud, Profanity, and Emergency detection</li>
<li>Urgency detection in customer support</li>
</ul>
</li>
</ul>
<h2 id="2-Machine-Learning"><a href="#2-Machine-Learning" class="headerlink" title="2. Machine Learning"></a>2. Machine Learning</h2><blockquote>
<p>å‚è€ƒé“¾æ¥ï¼š</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.lexalytics.com/technology/text-analytics/">https://www.lexalytics.com/technology/text-analytics/</a></li>
</ul>
</blockquote>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816152416947.png" alt="image-20220816152416947"></p>
<p><strong>machine learning for NLP</strong></p>
<ul>
<li>Processing natural language text is complex, and the traditional rules-based, explicit programming is not practical</li>
<li>Machine Learning allows <strong>algorithms</strong> to <strong>iteratively learn</strong> from text and extract rules, instead of explicitly programming for it</li>
<li>Machine learning can improve, accelerate and automate NLP tasks and text analytics functions</li>
</ul>
<p>Core NLP tasks are performed with machine learning models:</p>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/1-1024x773.png" alt="img"></p>
<p>Machine Learning Process:CRISP-DM</p>
<ul>
<li>The Cross-Industry-StandardProcess-for-Data-Mining (<strong>CRISP-DM</strong>), a well-established scientific method</li>
<li>Process Steps are:<ul>
<li>Business Understanding</li>
<li>Data Understanding</li>
<li>Data Preparation</li>
<li>Modeling</li>
<li>Evaluation</li>
<li>Deployment</li>
</ul>
</li>
</ul>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/CRISP-DM.png" alt="CRISP DM"></p>
<p><strong>Model Building &amp; Evaluation Deep Dive</strong></p>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816154743234.png" alt="image-20220816154743234"></p>
<h2 id="3-Classification-Modeling-Example"><a href="#3-Classification-Modeling-Example" class="headerlink" title="3. Classification Modeling Example"></a>3. Classification Modeling Example</h2><p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816155427886.png" alt="image-20220816155427886"></p>
<p><strong>model evaluation-Confusion Matrix</strong></p>
<p>For the classification problem,you predict either right or wrong-however,there are two dimensions to evaluate the result.</p>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816160721419.png" alt="image-20220816160721419"></p>
<p>#Correct Predictions/Total # Predictions Ability to find all relevant cases within</p>
<p>dataset. Tells us how complete the result is Ability to find only the relevant data points. Tells us how valid the result is</p>
<p>Harmonic mean of recall and precision </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Metric</th>
<th>Definition</th>
<th>Calculations using confusion matrix</th>
</tr>
</thead>
<tbody>
<tr>
<td>Accuracy</td>
<td>#Correct Predictions/Total # Predictions</td>
<td>$(TP+TN)/Total$</td>
</tr>
<tr>
<td>Recall(sensitivity)</td>
<td>Ability to find all relevant cases within dataset. Tells us how complete the result is</td>
<td>$(TP)/(TP+FN)$</td>
</tr>
<tr>
<td>Precision</td>
<td>Ability to find only the relevant data points. Tells us how valid the result is</td>
<td>$(TP)/(TP+FP)$</td>
</tr>
<tr>
<td>F1-Score</td>
<td>Harmonic mean of recall and precision</td>
<td>$F1=2\times \frac{\text{precision}\times\text{recall}}{\text{precision}+\text{recall}}$</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>An example:</p>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816162226429.png" alt="image-20220816162226429"></p>
</blockquote>
<h2 id="4-scikit-learn"><a href="#4-scikit-learn" class="headerlink" title="4. scikit-learn"></a>4. scikit-learn</h2><p>å®‰è£…ï¼š</p>
<figure class="highlight cmd"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install scikit-learn</span><br></pre></td></tr></tbody></table></figure>
<p>Scikit usage steps(5 high level steps)</p>
<ol>
<li><p>select a model(estimator object)</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">model = LinearRegression(normalize = <span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>Split the data into test and training sets</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = <span class="number">0.3</span>)</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>Train/fit the model</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train, t_train)</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>Predict Labels for test data</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predictions = model.predict(X_test)</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>Evaluate model(create metrics)</p>
</li>
</ol>
<p><strong>sklearn demo</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">'./smsspamcollection.tsv'</span>, sep=<span class="string">'\t'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816182518300.png" alt="image-20220816182518300"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æŸ¥çœ‹æ˜¯å¦æœ‰ç¼ºå¤±çš„æ•°æ®</span></span><br><span class="line">df.isnull().<span class="built_in">sum</span>()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816182535097.png" alt="image-20220816182535097"></p>
<p>å¯ä»¥çœ‹åˆ°æ­¤æ—¶æ•°æ®é›†ä¸­æ²¡æœ‰ç©ºç¼ºé¡¹ï¼Œå¦‚æœæœ‰ç©ºç¼ºé¡¹ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„ä»£ç è¡¥å……ç©ºç¼ºï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">blanks = []  <span class="comment"># start with an empty list</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> index,label,review <span class="keyword">in</span> df.itertuples():   <span class="comment"># iterate over the DataFrame</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(review)==<span class="built_in">str</span>:                    <span class="comment"># avoid NaN values</span></span><br><span class="line">        <span class="keyword">if</span> review.isspace():                 <span class="comment"># test 'review' for whitespace</span></span><br><span class="line">            blanks.append(index)             <span class="comment"># add matching index to list</span></span><br><span class="line">        </span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(blanks), <span class="string">'blanks: '</span>, blanks)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220817160730279.png" alt="image-20220817160730279"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(df[<span class="string">'label'</span>].unique())       <span class="comment"># æŸ¥çœ‹ç±»åˆ«</span></span><br><span class="line"><span class="built_in">print</span>(df[<span class="string">'label'</span>].value_counts()) <span class="comment"># æŸ¥çœ‹æ¯ä¸ªç±»åˆ«å¯¹åº”çš„æ•°æ®æ•°é‡</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816182554421.png" alt="image-20220816182554421"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å¯¹æŸä¸€æ•°æ®è¿›è¡Œç»Ÿè®¡æè¿°</span></span><br><span class="line">df[<span class="string">'length'</span>].describe()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816182620447.png" alt="image-20220816182620447"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = df[[<span class="string">'length'</span>,<span class="string">'punct'</span>]]</span><br><span class="line">y = df[<span class="string">'label'</span>]</span><br><span class="line"><span class="built_in">print</span>(X.shape)</span><br><span class="line"><span class="built_in">print</span>(y.shape)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816182659450.png" alt="image-20220816182659450"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># åˆ’åˆ†æ•°æ®é›†å’ŒéªŒè¯é›†</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, </span><br><span class="line">                                                    test_size=<span class="number">0.33</span>, </span><br><span class="line">                                                    random_state=<span class="number">42</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Training Data Shape:'</span>, X_train.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Testing Data Shape: '</span>, X_test.shape)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816182719587.png" alt="image-20220816182719587"></p>
<p><strong>åˆ›å»ºä¸€ä¸ªLogisticRegressionæ¨¡å‹</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># å»ºç«‹æ¨¡å‹</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">lr_model = LogisticRegression(solver=<span class="string">'lbfgs'</span>)</span><br><span class="line">lr_model.fit(X_train, y_train)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816182740106.png" alt="image-20220816182740106"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="comment"># åˆ›å»ºä¸€ä¸ªé¢„æµ‹</span></span><br><span class="line">predictions = lr_model.predict(X_test)</span><br><span class="line"><span class="comment"># è¾“å‡ºæ··æ·†çŸ©é˜µ</span></span><br><span class="line"><span class="built_in">print</span>(metrics.confusion_matrix(y_test,predictions))</span><br><span class="line"><span class="comment"># è¾“å‡ºåˆ†ç±»æŠ¥å‘Š</span></span><br><span class="line"><span class="built_in">print</span>(metrics.classification_report(y_test,predictions))</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816182819923.png" alt="image-20220816182819923"></p>
<p> <strong>åˆ›å»ºä¸€ä¸ªæœ´ç´ è´å¶æ–¯æ¨¡å‹naÃ¯ve Bayes classifier</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train a naÃ¯ve Bayes classifier:</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line">nb_model = MultinomialNB()</span><br><span class="line">nb_model.fit(X_train, y_train)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">predictions = nb_model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(metrics.confusion_matrix(y_test,predictions))</span><br><span class="line"><span class="built_in">print</span>(metrics.classification_report(y_test,predictions))</span><br><span class="line"><span class="built_in">print</span>(metrics.accuracy_score(y_test,predictions))</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816182908561.png" alt="image-20220816182908561"></p>
<p> <strong>åˆ›å»ºä¸€ä¸ªæ”¯æŒå‘é‡æœºæ¨¡å‹svc</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line">svc_model = SVC(gamma=<span class="string">'auto'</span>)</span><br><span class="line">svc_model.fit(X_train,y_train)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">predictions = svc_model.predict(X_test)</span><br><span class="line"><span class="built_in">print</span>(metrics.confusion_matrix(y_test,predictions))</span><br><span class="line"><span class="built_in">print</span>(metrics.classification_report(y_test,predictions))</span><br><span class="line"><span class="built_in">print</span>(metrics.accuracy_score(y_test,predictions))</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816183001913.png" alt="image-20220816183001913"></p>
<h2 id="5-Text-Feature-Extraction"><a href="#5-Text-Feature-Extraction" class="headerlink" title="5. Text Feature Extraction"></a>5. Text Feature Extraction</h2><ul>
<li>Machine learning algorithms (models) need numerical features to perform learning and prediction activities</li>
<li>We need to <strong>extract</strong> numerical features from the raw text </li>
</ul>
<h3 id="5-1-count-vectorization"><a href="#5-1-count-vectorization" class="headerlink" title="5.1 count vectorization"></a>5.1 count vectorization</h3><p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816183842413.png" alt="image-20220816183842413"></p>
<p><code>CountVectorizer</code> ç±»ï¼š</p>
<ul>
<li>create a matrix of counts,with columns as wordsï¼Œä¼šå°†æ–‡æœ¬ä¸­çš„è¯è¯­è½¬æ¢ä¸ºè¯é¢‘çŸ©é˜µ</li>
<li>çŸ©é˜µä¸­åŒ…å«ä¸€ä¸ªå…ƒç´ <code>a[i][j]</code>ï¼Œå®ƒè¡¨ç¤º<code>j</code>è¯åœ¨<code>i</code>ç±»æ–‡æœ¬ä¸‹çš„è¯é¢‘ã€‚This sparse matrix is called Document Term Matrix(DTM)</li>
<li><code>fit_transform</code>å‡½æ•°è®¡ç®—å„ä¸ªè¯è¯­å‡ºç°çš„æ¬¡æ•°</li>
<li><code>get_feature_names()</code>å¯è·å–è¯è¢‹ä¸­æ‰€æœ‰æ–‡æœ¬çš„å…³é”®å­—ï¼Œ</li>
<li><code>toarray()</code>å¯çœ‹åˆ°è¯é¢‘çŸ©é˜µçš„ç»“æœã€‚</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Scikit-learn's CountVectorizer</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">count_vect = CountVectorizer()</span><br><span class="line">X_train_counts = count_vect.fit_transform(X_train)</span><br><span class="line">X_train_counts.shape</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816185514946.png" alt="image-20220816185514946"></p>
<h3 id="5-2-Term-Frequency-TF"><a href="#5-2-Term-Frequency-TF" class="headerlink" title="5.2 Term Frequency(TF)"></a>5.2 Term Frequency(TF)</h3><ul>
<li>Term Frequency <code>tf(t,d)</code>-raw count of a term in a document, i.e.,the number of times a term <code>t</code> occurs in document <code>d</code></li>
<li>However,Term Frequency alone is not enough for a thorough feature analysis of the text. Consider stop words like â€œaâ€ or â€œtheâ€ </li>
<li>Because the term â€œtheâ€ is so common, term frequency will tend to incorrectly emphasize documents which happen to use the word â€œtheâ€ more frequently, without giving enough weight to the more meaningful terms â€œredâ€ and â€œdogsâ€.</li>
</ul>
<h3 id="5-3-Inverse-Document-Frequency-IDF"><a href="#5-3-Inverse-Document-Frequency-IDF" class="headerlink" title="5.3 Inverse Document Frequency(IDF)"></a>5.3 Inverse Document Frequency(IDF)</h3><ul>
<li>In order to reduce the unwanted impact of common words, an <strong>inverse document frequency</strong> factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely</li>
<li>It is the logarithmically scaled inverse fraction of the documents that contain the word (obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient)</li>
</ul>
<h3 id="5-4-TF-IDF"><a href="#5-4-TF-IDF" class="headerlink" title="5.4 TF-IDF"></a>5.4 TF-IDF</h3><ul>
<li>TF-IDF = Term Frequency * (1/document Frequency)</li>
<li>TF-IDF = Term Frequency * Inverse Document Frequency</li>
</ul>
<script type="math/tex; mode=display">
\begin{align}

\operatorname{tfidf}(t, d, D) & = \operatorname{tf}(t, d) \cdot \operatorname{idf}(t, D) \\
\operatorname{idf}(t, D) & = \log \frac{N}{|\{d \in D: t \in d\}|}

\end{align}</script><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Transform Counts to Frequencies with Tf-idf</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</span><br><span class="line">tfidf_transformer = TfidfTransformer()</span><br><span class="line">X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)</span><br><span class="line">X_train_tfidf.shape</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816185638730.png" alt="image-20220816185638730"></p>
<h3 id="5-5-TF-IDF-Vectorizer"><a href="#5-5-TF-IDF-Vectorizer" class="headerlink" title="5.5 TF-IDF Vectorizer"></a>5.5 TF-IDF Vectorizer</h3><ul>
<li>TF-IDF Vectorizer is superior to raw Count Vectorizer</li>
<li>TF-IDF allows us to understand the context of words across an <strong>entire corpus of documents</strong>, instead of just its relative importance in a single document</li>
<li>Scikit-Learnâ€™s <code>TfidfVectorizer</code> to train and fit our models </li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">vectorizer = TfidfVectorizer()</span><br><span class="line">X_train_tfidf = vectorizer.fit_transform(X_train) </span><br><span class="line"><span class="comment"># ä½¿ç”¨åŸå§‹çš„X_train</span></span><br><span class="line">X_train_tfidf.shape</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220816185819948.png" alt="image-20220816185819948"></p>
<h3 id="5-6-Pipline"><a href="#5-6-Pipline" class="headerlink" title="5.6 Pipline"></a>5.6 Pipline</h3><ul>
<li>Pipeline of transforms with a final estimator.</li>
<li>Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be â€˜transformsâ€™, that is, they must implement <code>fit</code> and <code>transform</code> methods. The final estimator only needs to implement <code>fit</code>. The transformers in the pipeline can be cached using <code>memory</code> argument.</li>
<li>The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a <code>'__'</code>, as in the example below. A stepâ€™s estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting it to <code>'passthrough'</code> or <code>None</code>.</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">text_clf = Pipeline([(<span class="string">'tfidf'</span>, TfidfVectorizer()), (<span class="string">'clf'</span>, LinearSVC())])</span><br><span class="line"><span class="comment"># Feed the training data through the pipeline</span></span><br><span class="line">text_clf.fit(X_train, y_train)  </span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220817145239824.png" alt="image-20220817145239824"></p>
<p>å¯¹é¢„æµ‹ç»“æœè¿›è¡Œè¯„ä¼°éªŒè¯ï¼Œå¯ä»¥å‘ç°æ­¤æ—¶å‡†ç¡®åº¦æœ‰æ˜æ˜¾çš„æå‡ï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">predictions = text_clf.predict(X_test)    <span class="comment"># é¢„æµ‹</span></span><br><span class="line"><span class="built_in">print</span>(metrics.confusion_matrix(y_test,predictions))      <span class="comment"># æ··æ·†çŸ©é˜µ</span></span><br><span class="line"><span class="built_in">print</span>(metrics.classification_report(y_test,predictions)) <span class="comment"># é¢„æµ‹æŠ¥å‘Š</span></span><br><span class="line"><span class="built_in">print</span>(metrics.accuracy_score(y_test,predictions))        <span class="comment"># é¢„æµ‹å‡†ç¡®åº¦</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220817150011864.png" alt="image-20220817150011864"></p>
<p>ä½¿ç”¨è¯¥ç®—æ³•é¢„æµ‹ä¸¤ä¸ªå¥å­ï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">text_clf.predict([<span class="string">"Hello, how are you?"</span>])</span><br><span class="line">text_clf.predict([<span class="string">'Congratulations, you have won $1 M'</span>])</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220817151218786.png" alt="image-20220817151218786"></p>
<p>Text Classification Project:</p>
<ul>
<li>Read in a collection of documents - a <strong>corpus</strong></li>
<li>Transform text into numerical vector data using a pipeline</li>
<li>Create a classifier</li>
<li>Fit/train the classifier</li>
<li>Test the classifier on new data</li>
<li>Evaluate performance</li>
</ul>
<h2 id="6-Semantic-Analysis"><a href="#6-Semantic-Analysis" class="headerlink" title="6. Semantic Analysis"></a>6. Semantic Analysis</h2><h3 id="6-1-æ¦‚è¿°"><a href="#6-1-æ¦‚è¿°" class="headerlink" title="6.1 æ¦‚è¿°"></a>6.1 æ¦‚è¿°</h3><ul>
<li>Semantic analysis is the process of drawing <strong>meaning</strong> from natural language text</li>
<li>It attempts to mimic the process humans follow, i.e., processing words in the <strong>context</strong> of their appearance, <strong>relating</strong> them with other words, and selecting most appropriate meaning (removing ambiguities)</li>
<li>Context plays an important role, it helps to attribute the correct meaning </li>
<li>It is an essential sub-task of NLP and the driving force behind machine learning tools such as chatbots, search engines and text analysis</li>
</ul>
<p><strong>How does Semantic Analysis work?</strong></p>
<ul>
<li>Semantic analysis begins by understanding relationships between lexical items (words, phrasal verbs, noun phrases, etc.)</li>
<li>It creates lexical hierarchies using: <ul>
<li><strong>Hyponyms/Hypernyms</strong>-inheritance like structure</li>
<li><strong>Meronomy</strong>-whole/part like structure</li>
<li><strong>Polysemy</strong>-relationship based on common core meaning </li>
<li><strong>Synonyms</strong>-words with same meaning and can substitute </li>
<li><strong>Antonyms</strong>-words with opposite meaning</li>
<li><strong>Homonyms</strong>-words with similar sound &amp; spelling but different meaning</li>
</ul>
</li>
<li>Semantic analysis considers signs and symbols (semiotics) and collocations (words that often go together)</li>
<li>Automated semantic analysis works with the help of machine learning algorithms. By feeding <strong>semantically enhanced</strong> algorithms with sample text,you can train machines to make accurate predictions based on past observations </li>
<li>Two important sub-tasks involved in this approach are:<ul>
<li><strong>Word sense disambiguation</strong> (e.g.,Orange could mean color,fruit, or a county in California) </li>
<li><strong>Relationship extraction</strong>(e.g.,relationship between persons, organizations, and places) </li>
</ul>
</li>
</ul>
<h3 id="6-2-Semantic-Analysis-Techniques"><a href="#6-2-Semantic-Analysis-Techniques" class="headerlink" title="6.2 Semantic Analysis Techniques"></a>6.2 Semantic Analysis Techniques</h3><hr>
<p><strong>Classification Models</strong></p>
<ul>
<li><strong>Topic Classification</strong>ï¼šSorting text to predefined <strong>topics</strong> that they belong to. For example, a service ticket could be regarding a â€œpayment issueâ€ or â€œshipping problemâ€.</li>
<li><strong>Sentiment Analysis</strong>ï¼šDetecting positive, negative, or neutral <strong>emotions</strong>. This could mean, for example, how customers feel about a product or service.</li>
<li><strong>Intent Classification</strong>ï¼šClassifying text based on what customers <strong>intend to do next</strong>. This could mean, for example, that customer wants to talk to an expert.</li>
</ul>
<hr>
<p><strong>Extraction Models</strong></p>
<ul>
<li><p><strong>Keyword Extraction</strong></p>
<ul>
<li>Finding relevant words and expressions in a text</li>
<li>Used for more granular insight,e.g.,a feedback classified as negative, what words or topics are mentioned most often</li>
</ul>
</li>
<li><p><strong>Entity Extraction</strong></p>
<ul>
<li>Identifying named entities in text. This could be customized to automatically detect company specific texts, e.g.,product/service names, ticket # etc.</li>
</ul>
</li>
</ul>
<h2 id="7-Word-Vectors"><a href="#7-Word-Vectors" class="headerlink" title="7. Word Vectors"></a>7. Word Vectors</h2><p><strong>What are word vectors?</strong></p>
<ul>
<li>A word vector is a set of <strong>numbers</strong> that <strong>represent</strong> the <strong>meaning</strong> of a word,with a lot of contextual information</li>
<li>A vector representing a word is an array of real-valued numbers, where each point captures a <strong>dimension</strong> of wordâ€™s meaning </li>
<li>These numbers encode the meaning of words in such a way that words close in vector space are expected to have similar meaning </li>
<li>Creation of word vectors is a critical component of semantic analysis, and this approach is called <strong>word embedding</strong></li>
</ul>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/0mRGKYujQkI7PcMDE.png" alt="img"></p>
<p><strong>Why word vectors?</strong></p>
<ul>
<li>Representing words with numbers enables mathematical operations: such as detecting (cosine) similarity, adding &amp;subtracting vectors, finding associations, and predicting meaning</li>
</ul>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220817172136226.png" alt="image-20220817172136226"></p>
<p>Interesting relationships can be established between word vectors.</p>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220817172222683.png" alt="image-20220817172222683"></p>
<p><strong>How are word vectors created?</strong></p>
<ul>
<li>Word vectors are created by feeding a large corpus of text into a deep learning model (neural network)</li>
<li>Word vectors are created by following distributional hypothesis, which states-<strong>â€œYou shall know a word by the company it keepsâ€</strong> </li>
<li>Words that share similar context tend to have similar meaning. </li>
<li>Models either use context to predict a target word (CBOW method) or use a word to predict a target context (Skip-Gram method)</li>
<li><code>word2vec</code> is a two-layer neural network model </li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en_core_web_md'</span>)  <span class="comment"># ä½¿ç”¨å¤§æ¨¡å‹</span></span><br><span class="line">nlp(<span class="string">u'lion'</span>).vector                 <span class="comment"># è®¡ç®—è¯å‘é‡</span></span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220818230411686.png" alt="image-20220818230411686"></p>
<p>æŸ¥çœ‹å‘é‡çš„ç»´åº¦ï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">doc = nlp(<span class="string">u'lion'</span>)</span><br><span class="line"><span class="built_in">print</span>(doc.vector.shape)</span><br><span class="line"><span class="built_in">print</span>(doc.vocab.vectors.shape)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220818230855115.png" alt="image-20220818230855115"></p>
<p>æŸ¥çœ‹å¥å­çš„å‘é‡è¡¨ç¤ºï¼ŒåŒæ ·æ˜¯300ä¸ªç»´åº¦ï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">doc = nlp(<span class="string">u'The quick brown fox jumps over the lazy dog'</span>)</span><br><span class="line">doc.vector</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220818230939376.png" alt="image-20220818230939376"></p>
<p><strong>Identifying similar vectors</strong>ï¼šThe best way to expose vector relationships is through the <code>.similarity()</code> method of Doc tokens.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokens = nlp(<span class="string">u'dog cat monkey'</span>)</span><br><span class="line"><span class="keyword">for</span> token1 <span class="keyword">in</span> tokens:</span><br><span class="line">    <span class="keyword">for</span> token2 <span class="keyword">in</span> tokens:</span><br><span class="line">        <span class="built_in">print</span>(token1.text, token2.text, token1.similarity(token2))</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220818231849802.png" alt="image-20220818231849802"></p>
<p>Note that order doesnâ€™t matter. <code>token1.similarity(token2)</code> has the same value as <code>token2.similarity(token1)</code></p>
<p><strong>Vector norms</strong></p>
<p>Itâ€™s sometimes helpful to aggregate 300 dimensions into a Euclidian (L2) norm, computed as the square root of the sum-of-squared-vectors. This is accessible as the <code>.vector_norm</code> token attribute. Other helpful attributes include <code>.has_vector</code> and <code>.is_oov</code> or <strong>out of vocabulary</strong>.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tokens = nlp(<span class="string">u'dog cat nowaythere'</span>)</span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">    <span class="built_in">print</span>(token.text, token.has_vector, token.vector_norm, token.is_oov)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220818233555714.png" alt="image-20220818233555714"></p>
<p><strong>Vector arithmetic</strong></p>
<p>Believe it or not, we can actually calculate new vectors by adding &amp; subtracting related vectors. A famous example suggests</p>
<p></p><pre>"king" - "man" + "woman" = "queen"</pre><br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> spatial</span><br><span class="line">cosine_similarity = <span class="keyword">lambda</span> x, y: <span class="number">1</span> - spatial.distance.cosine(x, y)</span><br><span class="line">king = nlp.vocab[<span class="string">'king'</span>].vector</span><br><span class="line">man = nlp.vocab[<span class="string">'man'</span>].vector</span><br><span class="line">woman = nlp.vocab[<span class="string">'woman'</span>].vector</span><br><span class="line">queen = nlp.vocab[<span class="string">'queen'</span>].vector</span><br><span class="line">new_vector = king - man + woman</span><br><span class="line">cosine_similarity(new_vector, queen)</span><br><span class="line">cosine_similarity(new_vector, woman)</span><br></pre></td></tr></tbody></table></figure><p></p>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220818234152751.png" alt="image-20220818234152751"></p>
<h2 id="8-sentiment-analysis"><a href="#8-sentiment-analysis" class="headerlink" title="8. sentiment analysis"></a>8. sentiment analysis</h2><p><strong>What is sentiment analysis?</strong></p>
<ul>
<li>Sentiment analysis is a method that detects polarity (e.g.,positive or negative opinion) within the text</li>
<li>It is also used to detect emotions (happy, sad, angry, etc.), urgency (urgent vs. not urgent) and intentions (interested vs. not interested) </li>
<li>Sentiment analysis can be rule-based (manually crafted rules), automatic(feature extraction &amp; text classification),or hybrid </li>
<li>Sentiment analysis is hard due to multiple reasons - sarcasm, idioms, negation handling, adverbial modifiers, comparisons etc. </li>
</ul>
<p><strong>How is sentiment analysis used?</strong></p>
<ul>
<li>Since people share their opinion more openly than ever before, sentiment analysis is useful in a variety of ways such as: social media monitoring, customer support, customer feedback, brand monitoring, voice of customer, market research, etc.</li>
<li>Sentiment analysis can also be used as a real-time analysis tool, especially if events requiring urgent action need to be detected</li>
</ul>
<h2 id="9-VADER"><a href="#9-VADER" class="headerlink" title="9. VADER"></a>9. VADER</h2><ul>
<li>Valence Aware Dictionary for Sentiment Reasoning (VADER) is a model used for text sentiment analysis that is sensitive to both <strong>polarity</strong> (positive/negative) and <strong>intensity</strong> (strength) of emotion </li>
<li>Primarily, VADER sentiment analysis relies on a dictionary which maps lexical features to emotion intensities called <strong>sentiment scores</strong></li>
<li>The sentiment score of a text can be obtained by summing up the intensity of each word in the text</li>
<li>VADER is a rule-based system</li>
<li>VADER understands that words like â€œloveâ€, â€œlikeâ€, â€œenjoyâ€,â€œhappyâ€ all convey a <strong>positive</strong> sentiment.</li>
<li>VADER is intelligent enough to understand basic context of these words, such as â€œdid not loveâ€ as a negative sentiment.</li>
<li>It uses rules to also understand that the capitalization and punctuation enhance intensity of emotions, e.g., â€œLOVE!!!!â€ </li>
<li>VADER is available in the NLTK package and can be applied directly to unlabeled data.</li>
</ul>
<p><strong>Download the VADER lexicon.</strong> </p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line">nltk.download(<span class="string">'vader_lexicon'</span>)</span><br><span class="line"><span class="keyword">from</span> nltk.sentiment.vader <span class="keyword">import</span> SentimentIntensityAnalyzer</span><br><span class="line">sid = SentimentIntensityAnalyzer()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220819171742146.png" alt="image-20220819171742146"></p>
<p>VADERâ€™s <code>SentimentIntensityAnalyzer()</code> takes in a string and returns a dictionary of scores in each of four categories:</p>
<ul>
<li>negative</li>
<li>neutral</li>
<li>positive</li>
<li>compound <strong>(computed by normalizing the scores above)</strong></li>
</ul>
<p>è®¡ç®—æ¯ä¸ªå¥å­ä¸­åŒ…å«çš„æƒ…æ„Ÿæ¯”ä¾‹ï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="string">'This was a good movie.'</span></span><br><span class="line">sid.polarity_scores(a)</span><br><span class="line">a = <span class="string">'This was the best, most awesome movie EVER MADE!!!'</span></span><br><span class="line">sid.polarity_scores(a)</span><br><span class="line">a = <span class="string">'This was the worst film to ever disgrace the screen.'</span></span><br><span class="line">sid.polarity_scores(a)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220819171855755.png" alt="image-20220819171855755"></p>
<p><strong>Adding Scores and Labels to the DataFrame</strong></p>
<p>åœ¨<code>score</code>ååŠ å…¥ä¸€åˆ—<code>review</code>æ•°æ®</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'scores'</span>] = df[<span class="string">'review'</span>].apply(<span class="keyword">lambda</span> review: sid.polarity_scores(review))</span><br><span class="line">df.head()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220819173450036.png" alt="image-20220819173450036"></p>
<h2 id="10-Sentiment-Analysis-Project"><a href="#10-Sentiment-Analysis-Project" class="headerlink" title="10. Sentiment Analysis Project"></a>10. Sentiment Analysis Project</h2><p>For this project, weâ€™ll perform the same type of NLTK VADER sentiment analysis, this time on our movie reviews dataset.</p>
<p>The 2,000 record IMDb movie review database is accessible through NLTK directly with</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> movie_reviews</span><br></pre></td></tr></tbody></table></figure>
<p>However, since we already have it in a tab-delimited file weâ€™ll use that instead.</p>
<p><strong>Load the Data</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">'../TextFiles/moviereviews.tsv'</span>, sep=<span class="string">'\t'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220819174922267.png" alt="image-20220819174922267"></p>
<p><strong>Remove Blank Records (optional)</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># REMOVE NaN VALUES AND EMPTY STRINGS:</span></span><br><span class="line">df.dropna(inplace=<span class="literal">True</span>)</span><br><span class="line">blanks = []  <span class="comment"># start with an empty list</span></span><br><span class="line"><span class="keyword">for</span> i,lb,rv <span class="keyword">in</span> df.itertuples():  <span class="comment"># iterate over the DataFrame</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(rv)==<span class="built_in">str</span>:            <span class="comment"># avoid NaN values</span></span><br><span class="line">        <span class="keyword">if</span> rv.isspace():         <span class="comment"># test 'review' for whitespace</span></span><br><span class="line">            blanks.append(i)     <span class="comment"># add matching index numbers to the list</span></span><br><span class="line">df.drop(blanks, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220819174948622.png" alt="image-20220819174948622"></p>
<p><strong>Import <code>SentimentIntensityAnalyzer</code> and create an sid object</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.sentiment.vader <span class="keyword">import</span> SentimentIntensityAnalyzer</span><br><span class="line">sid = SentimentIntensityAnalyzer()</span><br></pre></td></tr></tbody></table></figure>
<p><strong>Use sid to append a <code>comp_score</code> to the dataset</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'scores'</span>] = df[<span class="string">'review'</span>].apply(<span class="keyword">lambda</span> review: sid.polarity_scores(review))</span><br><span class="line">df[<span class="string">'compound'</span>]  = df[<span class="string">'scores'</span>].apply(<span class="keyword">lambda</span> score_dict: score_dict[<span class="string">'compound'</span>])</span><br><span class="line">df[<span class="string">'comp_score'</span>] = df[<span class="string">'compound'</span>].apply(<span class="keyword">lambda</span> c: <span class="string">'pos'</span> <span class="keyword">if</span> c &gt;=<span class="number">0</span> <span class="keyword">else</span> <span class="string">'neg'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220819175107094.png" alt="image-20220819175107094"></p>
<p><strong>Perform a comparison analysis between the original <code>label</code> and <code>comp_score</code></strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report,confusion_matrix</span><br><span class="line">accuracy_score(df[<span class="string">'label'</span>],df[<span class="string">'comp_score'</span>])</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220819175210237.png" alt="image-20220819175210237"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(classification_report(df[<span class="string">'label'</span>],df[<span class="string">'comp_score'</span>]))</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220819175221230.png" alt="image-20220819175221230"></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(confusion_matrix(df[<span class="string">'label'</span>],df[<span class="string">'comp_score'</span>]))</span><br></pre></td></tr></tbody></table></figure>
<p><img src="/2022/08/16/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%A6%82%E8%BF%B0/image-20220819175312781.png" alt="image-20220819175312781"></p>

        </div>

        
            <section class="post-copyright">
                
                
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E7%BC%96%E7%A8%8B/">ğŸ·ï¸ç¼–ç¨‹</a>
                    
                        <a href="/tags/python/">ğŸ·ï¸python</a>
                    
                        <a href="/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/">ğŸ·ï¸è‡ªç„¶è¯­è¨€å¤„ç†</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>Â· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/2022/07/23/python%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%B7%A5%E5%85%B7/">NLPåŸºç¡€å·¥å…·</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>Â© å°¹ç¥ºç¿” | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>